{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intro to Supervised Learning:\n",
    "    Random Variable\n",
    "    ---------------\n",
    "    Bernoulli   P(H) = P\n",
    "                P(T) = 1 - P\n",
    "    \n",
    "    Discrete: X be r.v.\n",
    "                P(x = 1) = P\n",
    "                P(x = 0) = 1 - P\n",
    "    \n",
    "    Continuous Normal / Gaussian\n",
    "    -----------------------------\n",
    "    density f(x) = (1 / ( sqrt( 2 * pi ) * sigma ) ) * e ^ -(( x - mu ) ^ 2 / (2 * sigma ^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli - P ?\n",
    "= P(H)\n",
    "\n",
    "HTTTH\n",
    "\n",
    "E[x] = P * 1 + (1 - p) * 0 = P ~~ (2 / 5)\n",
    "\n",
    "Maximum Likelihood Estimation (MLE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised Learning:\n",
    "    E(y) = f(x)\n",
    "label             feature vector\n",
    "(0/1,           (What we use to predict y like sq. ft, zipcode, etc. for house price)\n",
    "click/no click,\n",
    "could be continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generalized Linear Model (GML):\n",
    "    0 = Ao + A1X1 + ... + AnXn\n",
    "    y ~ g(0)\n",
    "    where x are independent variables or \"predictors\"\n",
    "\n",
    "Logistic Regression (Modeling for Bernoulli Random Variables):\n",
    "    Passing our features to the logistic regression function, we get back a Bernoulli random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maximum Likelihood:\n",
    "    Given a PD and data, choose the parameters that maximize the probability of the data conditioned on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent:\n",
    "    0^(n+1) = 0^n - dt * delta f(0^n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFlow\n",
    "----------\n",
    "\n",
    "- Numerical Linear Algebra Library using GPUs\n",
    "- Automated differentiation\n",
    "    \"differentiable code\"\n",
    "\n",
    "Keras\n",
    "-----\n",
    "\n",
    "- Wrapper for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common Task Framework:\n",
    "    - Split data into 3 parts: training, test, and validation\n",
    "    - You are given variables for all data but only labels for the training dataset\n",
    "    - Build a model to the train dataset, predict on test and validation\n",
    "    - You can only see performance on the test set until the competition is over (to prevent over-fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area under the curve (AUC):\n",
    "    - Equivalent to the % of pairs of samples correctly sorted by the model\n",
    "    - Allows for the entire range of FP thresholds to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other Metrics:\n",
    "    - Accuracy (% of samples correctly classified for a chosen threshold)\n",
    "    - Precision (% of predicted positives that are actually positives)\n",
    "    - Recall (% of positives that are actually positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep Learning:\n",
    "    Feed Forward Neural Networks\n",
    "        - Every value in the matrix creates a connection between the next value\n",
    "        - y = sigma(W2h + b2) & h = sigma(W1x + b1)\n",
    "        - Whereas in Logistic Regression W is M x 1, now W is M x N\n",
    "        - sigma(x) = ReLU(x) = max(x, 0)\n",
    "        - Train w/ GD (use chain rule or \"back propagation\" for the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoders:\n",
    "    - Input layer is larger than the hidden layer, and the output layer is often the transpose of the input layer.\n",
    "    - This architecture allows you to obtain new features while also compressing data where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convolution Neural Network:\n",
    "    - By the time a network has trained nodes to recognize say the whiskers of a cat, it will activate when a cat image is fed into the CNN.\n",
    "    - You apply a series of small filters to images in a CNN. These filters individually seek to recognize patterns in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Google will actually release models into the world, allowing users to avoid having to train a network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Augmentation:\n",
    "    You can rotate, scale, and transpose images to form new images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
