{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Classification\n",
    "2. Clustering\n",
    "3. Dimension Reduction:\n",
    "    Latent Representation\n",
    "4. Sequence Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rows -> Data Instance\n",
    "Columns -> Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Applying for a credit card:\n",
    "    Application 1 -> Credit Score 1, Car Color 1, ...\n",
    "    Application 2 -> Credit Score 2, Car Color 2, ...\n",
    "    \n",
    "Each row represents a vector of feature instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note: You can numericize all feature instances.\n",
    "\n",
    "Once you have some feature vectors, you can use them to form a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this case, a 1/0 represents a yes/no regarding the acceptance of a given credit card application.\n",
    "\n",
    "We want a function f(x) -> y where y is {0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training data:\n",
    "    D = { (Xi, Yi) } where 1 <= i <= n \n",
    "    \n",
    "    OR\n",
    "    \n",
    "    D -> [Learning Algorithm L] -> Model M -> 0/1\n",
    "    ---------------Offline Tank------------------\n",
    "    \n",
    "Once a model is built, data x can be sent to the model to return y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Different types of classification:\n",
    "\n",
    "f: x - y -> {0, 1} (binary classification)\n",
    "    \n",
    "f: x - y -> {Category Variable} (Multi-Class Classification)\n",
    "\n",
    "f: x - y -> {Continuous Value} (Regression Problem)\n",
    "    \n",
    "Each of these are collectively called Supervised Learning. This is because we have both X & Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regarding the learning algorithms employed, you can commonly find and use:\n",
    "    Bayesian Classification,\n",
    "    Naive Bayes Classification,\n",
    "    Support Vector Machines,\n",
    "    Decision Trees (which form Random Forests),\n",
    "    Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The fit function in SciKit-Learn will perform the learning algorithm on a dataset.\n",
    "\n",
    "The accuracy of a model can be described as the following (with a two-class classification):\n",
    "    \n",
    "    Confusion Matrix\n",
    "        0   1  Sum\n",
    "    0  20  15 = 35\n",
    "    \n",
    "    1  10  55 = 65\n",
    "    \n",
    "  Sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Data:\n",
    "    D = 100,\n",
    "    Dy = 70,\n",
    "    Dn = 30\n",
    "    \n",
    "    -> Model m\n",
    "    \n",
    "    -> Dy = 65,\n",
    "       Dn = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For an n-class classification problem, the confusion matrix will be n^2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True Negative (TN) -> I predicted it as negative, and my prediction was true.\n",
    "False Negative (FN) -> I predicted it as negative, but my prediction was false.\n",
    "\n",
    "True Positive (TP) -> I predicted it as positive, and my prediction was true.\n",
    "False Positive (FP) -> I predicted it as positive, but my prediction was false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = (20 + 55) / 100 = 0.75.\n",
    "\n",
    "Precision = TP / (TP + FP) = 55 / (55 + 10) = (11 / 13). \n",
    "This deals with the positive class. \n",
    "(i.e. What fraction of things that are being predicted as positive are actually positive?)\n",
    "\n",
    "Recall = TP / (TP  + FN) = 55 / (55 + 15) = (11 /14).\n",
    "This deals with the\n",
    "(i.e. What fraction of actual positive are predicted as positive?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the precision of 3+ class classification, convert the confusion matrix into a smaller 2x2 matrix:\n",
    "\n",
    "1. Select a primary class to evaluate.\n",
    "2. Relabel all other classes as a single grouped class.\n",
    "3. Perform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In a two-class classification problem, the default class of precision is the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers can become unbalanced from aggression. Think of spam classification:\n",
    "    Precision goes down if too many non-spam emails are classified as spam.\n",
    "\n",
    "Classifier calibration refers to a balancing between the precision and recall of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (2 * Pr * Re) / (Pr + Re). The harmonic mean is referred to as \"f1\". Good values of \"f1\" are close to 0.5 or where the denominator is as large as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Problem:\n",
    "    f: x -> y (where y is continuous)\n",
    "        Xi -> *Yi (Prediction)\n",
    "               Yi (Actual)\n",
    "            \n",
    "    RMSE (root mean square error) = sqrt( (sum(Yi - *Yi)^2) / n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering (Unsupervised Learning)\n",
    "D = { Xi } where i <= i <= n.\n",
    "\n",
    "Given a dataset, group the data into n-clusters.\n",
    "The result will be a class label for each datum.\n",
    "\n",
    "Many times, the label is not given. It must be derived in a Gaussian distribution context.\n",
    "\n",
    "In other words, we want to learn the distribution of x (i.e. P(x) compared to P(x, y) for classification).\n",
    "\n",
    "Clustering is much easier with a convex (circular) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K-Means:\n",
    "    Given a 2D dataset, say k=3,\n",
    "        1. Pick 3 random points/vectors to be a mean.\n",
    "        2. Assign each point to a cluster (i.e. For each point, compute its distance to each of the three means)\n",
    "        3. Pick a point in each cluster to be the mean of that cluster based on the mean of that dataset.\n",
    "        4. Repeat Step 2 until a stopping condition is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian Mixture Clustering is also called Gaussian Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dimension Reduction:\n",
    "    Given an n x d dataset, we can learn the best representation of that data.\n",
    "    \n",
    "    - Bag-of-Words(BOW) representation of a document. In BOW, each of the words or phrases becomes a feature.\n",
    "      In doing BOW, you lose the sequence information (for context within a sentence).\n",
    "      The frequency of a particular word or phrase in a document is called word frequency.\n",
    "      In certain domains, the frequency of some words might be sparse.\n",
    "    \n",
    "    - The curse of dimensionality is the sense in which the \"ratio between the nearest neighbor (NN) and the farthest neighbor (FN) is 1\".\n",
    "      This often occurs with high-dimensional data.\n",
    "    - In reducing the dimensionality of a dataset,\n",
    "        1. The number of columns in a dataset will be reduced\n",
    "        2. The latent features of a dataset will be preserved\n",
    "        3. The dataset's sparsity will be decreased\n",
    "    - In short, reduction asks for Rd -> Rl.\n",
    "    - A widely-used mode of dimension reduction is called Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA:\n",
    "    Given an n x d matrix (D), \n",
    "    -> Co-variance matrix (d x d)\n",
    "    -> Sigma (d x d) = U1 ^ U2, all with dimensionality of d x d.\n",
    "    U1 = U = First Principle Component\n",
    "    U2 = U,^T = Second Principle Component\n",
    "    \n",
    "The direction of the eigenvectors of a dataset gives you the highest variance in the dataset.\n",
    "\n",
    "Principle Component Analysis: Projecting the data to a space such that the largest amount of data variance is captured via the eigenvector directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD (Singular Value Decomposition) - Best with Orthogonal Data\n",
    "\n",
    "D = L ^ R^T\n",
    "\n",
    "D (n x d) = (n x k) * (k x k) * (k x d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence Data Modeling:\n",
    "    \n",
    "    - Take an English sentence for example: \"I am taking this course\"\n",
    "    - HMM (Hidden Markov Model): HTHHHTTHTTHHH\n",
    "                                 FBFFBF\n",
    "    - NLP uses Hidden Markov Models.\n",
    "    - spaCy as well as Word2Vec and Gensim is industrial strength NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph Theory:\n",
    "    Given a large graph, predict the value of missing links in the graph domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group Link Prediction (Project 1):\n",
    "    E_j composed of { a1, a2, ..., a_ej}, t_j\n",
    "    \n",
    "    Version 1: P(u | A, t) ?\n",
    "        -> u (a node)\n",
    "    Version 2: P(u | A, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project 2:\n",
    "\n",
    "Opiod Recovery Group Text Prediction:\n",
    "    1. Will a user relapse given their post frequency?\n",
    "    \n",
    "Dark Web Link Prediction:\n",
    "    1. Creating a link between drug vendors and public accounts using a HMM.\n",
    "    2. Creating a link between drug vendors and their bitcoin accounts.\n",
    "    3. P(V | price, time) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
