\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Scalable Gunshot Detection Systems with Convolutional Neural Networks\\
{\footnotesize }
\thanks{We gratefully acknowledge the support of NSF grant REU-1659488 which provided research stipends, travel funds, and supply money for our summer research project.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Alex Morehead}
\IEEEauthorblockA{\textit{Computer Science, Mathematics, \& Physics} \\
\textit{Missouri Western State University}\\
Saint Joseph, Missouri, USA \\
amorehead1@missouriwestern.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Lauren Ogden}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{Columbia University}\\
New York City, New York, USA \\
lao2125@columbia.edu}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Gabe Magee}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{Pomona College}\\
Claremont, California, USA \\
glma2016@pomona.edu}
\and
\IEEEauthorblockN{4\textsuperscript{th} Ryan Hosler}
\IEEEauthorblockA{\textit{Computer \& Information Science} \\
\textit{Indiana University-Purdue University Indianapolis}\\
Indianapolis, Indiana, USA \\
rjhosler@iu.edu}
\and
\IEEEauthorblockN{5\textsuperscript{th} George Mohler}
\IEEEauthorblockA{\textit{Computer \& Information Science} \\
\textit{Indiana University-Purdue University Indianapolis}\\
Indianapolis, Indiana, USA \\
gmohler@iupui.edu}
}

\maketitle

\begin{abstract}
Many cities with gunshot detection systems depend on expensive systems that rely on humans differentiating between gunshots and non-gunshots, such as ShotSpotter®. Thus, a scalable gunshot detection system that is low in cost and high in accuracy would be advantageous for a variety of cities across the globe, in that it would favorably promote the delegation of tasks typically worked by humans to machines. A convolutional neural network (CNN) was trained on a variety of sound data to recognize gunshots. This model was then deployed to a Raspberry Pi Model 3 B+ with an SMS modem attached, and the results were found to be advantageous. The findings generated by this research project have the potential to expand the current state of knowledge regarding sound-based applications of CNNs, and while simultaneously reducing the amount of jobs that require human input the results of this project could very well lead to an increase in safety standards for a city’s residents.
\end{abstract}

\begin{IEEEkeywords} machine learning, convolutional neural network, spectrogram, sound classification, microcomputer, smart city
\end{IEEEkeywords}

\section{Introduction}
Properly implementing a gunshot detection model to be used on a city-wide array of microcomputers enables automation of what previously required dedicated teams of human operators to perform. Further, it demonstrates the capabilities of deep learning architectures in sound classification from substantial amounts of sound data. Motivations such as these make it clear why this category of research is warranted along with our endeavors.

\section{The Data}

\subsection{Sources and Derivatives}

We obtained our data from two places: free internet  databases such as Freesound and a repository of sounds recorded using a microphone connected to a Raspberry Pi microcomputer. In addition to this, we used a generative adversarial network (GAN) as well as sound augmentations to create additional samples of gunfire sounds and to prevent our model from overfitting to our compiled dataset.

Further, we decidedly divided our data into three sets: training data, testing data, and validation data. This was to prevent our models from overfitting to our given dataset which had been occurring previously. Detailed below is a list of the augmentations we applied to each of our sound samples.

\begin{table}[htbp]
\centering
\caption{Data Augmentations}
\small
\begin{tabular}{*{3}{p{.285\linewidth}}}
\toprule
\textbf{Time Shift} & \textbf{Pitch Change} & \textbf{Speed Change} \\\midrule
Shifts a sound sample to the left or right by a randomly chosen amount less than 50\% of the length, and then fills in silence as needed. & Changes the pitch of a sample by a randomly-chosen factor between 70\% and 130\%. & Alters the playback speed of a sample by a randomly-chosen amount between 70\% and 130\%. \\\midrule
\end{tabular}
\label{tab1}
\end{table}

\begin{table}[htbp]
\centering
\small
\begin{tabular}{*{2}{p{.45\linewidth}}}
\toprule
\textbf{Volume Change} & \textbf{Background Noise Addition} \\\midrule
Increases the amplitude of a sample with a uniformly-random variable. & Introduces random background noise into a sample while making sure that no gunshots are added into a sample that does not originally contain a gunshot. \\\midrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
\caption{The data augmentations used on all sound samples.}
\label{fig1}
\end{figure}

\section{CNNs}
Convolutional Neural Networks (CNNs) are neural networks designed to locate, model, and accurately predict patterns present in input data such as a colored image. They do so by iteratively sliding over small regions of data and translating any inherent properties in a region over to a proceeding network layer. This process is repeated up until the output layer which generates a prediction.

\begin{figure}[h!]
    \begin{flushleft}
        \includegraphics[width=\linewidth]{{"2D CNN Architecture"}.png}
        \caption{An illustration of our 2D CNN's architecture.}
        \label{fig:illustration1}
    \end{flushleft}
\end{figure}

\section{Spectrograms}
Spectrograms are visual representations of the frequency and amplitude of sound over a specified span of time. For our project, we created two models, a 1D architecture that looks at sound represented as an array of frequency values and a 2D architecture that instead analyzes sound represented as spectrograms. For the time-series model each entry simply corresponds to a frequency measurement in a time-series, whereas for the spectrogram model the entry for each specific frequency-time cartesian coordinate is an amplitude value.

\section{Methodology}
\subsection{Training}

A convolutional neural network (CNN) was trained on a variety of sound data to recognize gunshots. While we had labels for sounds other than gunshots, we grouped them into a singular group “other”. Then each of these samples were preprocessed to turn them into spectrograms if the model was trained on them. The models were trained for 100 epochs or until the target metric of a training session, accuracy in this case, did not change for fifteen epochs.

\subsection{Deployment}
Each model was then deployed to a Raspberry Pi Model 3 B+ with an SMS modem attached. The models were loaded in as hierarchical data format (H5) files as opposed to their TensorFlow Lite counterparts for performance and accuracy concerns. Our program has three processes – one to put audio from a stream onto a queue, one to analyze sound data pulled from the queue, and one to send an SMS alert message to a predetermined list of phone numbers.

\section{Results}

We found that all our Keras models performed well on a validation set. The best model, however, was found to be a combination of using our two 2D models together by implementing a majority-rules algorithm which dispatches alerts if both models positively identified the sound of a gunshot.

\begin{table}[htbp]
\caption{Gunshot Detection Results}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
& \textbf{1D CNN} & \textbf{2D CNN (64)} & \textbf{2D CNN (128)} & \textbf{CNN Ensemble} \\
\cline{2-4}
\hline
Accuracy & 97.4\%    & 98.8\% & 98.8\% & 98.8\%  \\
\hline
Precision & 94.2\% & 96.5\% & 94.3\% & 97.1\% \\
\hline
Recall & 84.3\% & 93.8\% & 96.2\% & 92.9\% \\
\hline
F1 Score & 89.0\% & 95.1\% & 95.2\% & 95.0\% \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}

\begin{figure}[htbp]
\caption{The findings of a cross-evaluation technique applied to our models.}
\label{fig2}
\end{figure}

\section{Significance \& Future Work}

The findings generated by this research project have the potential to expand the current state of knowledge regarding sound-based applications of CNNs, and while simultaneously reducing the amount of jobs that require human input the results of this project could very well increase the standards of safety for a city’s residents. Ideally, a feature we would like to create in our pipeline in the future would allow for robust localization of gunshot alerts within a proposed cluster of Raspberry Pi units.

\section*{Acknowledgment}

We gratefully acknowledge the support of NSF grant REU-1659488 which provided research stipends, travel funds, and supply money for this summer research project. Thanks also to all faculty and support staff who helped coordinate this year's undergraduate research experience (REU) for data science at IUPUI.

\begin{thebibliography}{00}
\bibitem{b1} Piczak, Karol J. "Environmental sound classification with convolutional neural networks." In 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1-6. IEEE, 2015.
\bibitem{b2} Takahashi, Naoya, Michael Gygli, Beat Pfister, and Luc Van Gool. "Deep convolutional neural networks and data augmentation for acoustic event detection." arXiv preprint arXiv:1604.07160 (2016).
\bibitem{b3} Takahashi, Naoya, Michael Gygli, Beat Pfister, and Luc Van Gool. "Deep convolutional neural networks and data augmentation for acoustic event detection." arXiv preprint arXiv:1604.07160 (2016).
\bibitem{b4} Lim, Hyungui, Jeongsoo Park, and Y. Han. "Rare sound event detection using 1D convolutional recurrent neural networks." In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), pp. 80-84. 2017.
\bibitem{b5} Jaiswal, Kaustumbh, and Dhairya Kalpeshbhai Patel. "Sound Classification Using Convolutional Neural Networks." In 2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM), pp. 81-84. IEEE, 2018.
\bibitem{b6} DAmiriparian, Shahin, N. Cummins, S. Julka, and B. W. Schuller. "Deep convolutional recurrent neural network for rare acoustic event detection." In Proc. DAGA, pp. 1522-1525. 2018.
\bibitem{b7} Shi, Bowen, Ming Sun, Chieh-Chi Kao, Viktor Rozgic, Spyros Matsoukas, and Chao Wang. "Compression of acoustic event detection models with low-rank matrix factorization and quantization training." arXiv preprint arXiv:1905.00855 (2019).
\bibitem{b8} Prince, Peter, Andrew Hill, Evelyn Piña Covarrubias, Patrick Doncaster, Jake L. Snaddon, and Alex Rogers. "Deploying acoustic detection algorithms on low-cost, open-source acoustic sensors for environmental monitoring." Sensors 19, no. 3 (2019): 553.
\end{thebibliography}

\end{document}
