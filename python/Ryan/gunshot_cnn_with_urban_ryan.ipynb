{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Directory \n",
    "import glob\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "import librosa\n",
    "\n",
    "# Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "# Data Pre-processing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Input, layers, optimizers, backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import cv2\n",
    "\n",
    "# Configuration\n",
    "#py.init_notebook_mode(connected=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[]\n",
    "labels = []\n",
    "sample_slice_iteration = 0\n",
    "\n",
    "gunshot_sound_dir = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\gunshot\\\\\"\n",
    "\n",
    "for file in os.listdir(gunshot_sound_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        sample, sample_rate = librosa.load(gunshot_sound_dir + file)\n",
    "        if (sample.size <= 44100):\n",
    "            sample_slice = np.zeros(44100)\n",
    "            sample_slice[0:sample.size] = sample\n",
    "            label = 1\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "            \n",
    "        for i in range(0, sample.size - 44100, 44100):\n",
    "            sample_slice = sample[i : i + 44100]\n",
    "            label = 1\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "        \n",
    "glassbreak_sound_dir = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\glassbreak\\\\\"\n",
    "\n",
    "print(\"...Switching to glassbreak sounds...\")\n",
    "\n",
    "for file in os.listdir(glassbreak_sound_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        sample, sample_rate = librosa.load(glassbreak_sound_dir + file)\n",
    "        if (sample.size <= 44100):\n",
    "            sample_slice = np.zeros(44100)\n",
    "            sample_slice[0:sample.size] = sample\n",
    "            label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.5:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "            \n",
    "        for i in range(0, sample.size - 44100, 44100):\n",
    "            sample_slice = sample[i : i + 44100]\n",
    "            label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.5:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"samples.npy\", samples)\n",
    "np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\_project\\\\gunshot_detection\\\\Train\\\\\"\n",
    "dr = os.listdir(mypath)\n",
    "for i in range(len(dr)):\n",
    "    dr[i] = int(dr[i][:-4])\n",
    "dr = np.sort(dr)\n",
    "files = np.zeros(len(dr)).astype('str')\n",
    "for i in range(len(dr)):\n",
    "    files[i] = str(dr[i]) + '.wav'\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read in the csv file of descriptors for all other urban sounds\n",
    "sound_types = pd.read_csv(\"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\_project\\\\gunshot_detection\\\\train.csv\")\n",
    "print(sound_types.loc[0,'Class'])\n",
    "\n",
    "j=0\n",
    "count = 0\n",
    "#read in all of the wav files similar to above\n",
    "urban_sound_dir = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\_project\\\\gunshot_detection\\\\Train\\\\\"\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".wav\"):\n",
    "        count += 1\n",
    "        sample, sample_rate = librosa.load(urban_sound_dir + file)\n",
    "        if (count % 100 == 0):\n",
    "            print (count)\n",
    "        if (sample.size <= 44100):\n",
    "            sample_slice = np.zeros(44100)\n",
    "            sample_slice[0:sample.size] = sample\n",
    "            if(sound_types.loc[j, 'Class'] == \"gun_shot\"):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "        for i in range(0, sample.size - 44100, 44100):\n",
    "            sample_slice = sample[i : i + 44100]\n",
    "            if(sound_types.loc[j, 'Class'] == \"gun_shot\"):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "        j +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(np.load(\"samples.npy\"))\n",
    "labels = list(np.load(\"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_slice_iteration = 0\n",
    "\n",
    "sound_dir = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\_project\\\\gunshot_detection\\\\fireworks\\\\\"\n",
    "\n",
    "for file in os.listdir(sound_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        sample, sample_rate = librosa.load(sound_dir + file)\n",
    "        if (sample.size <= 44100):\n",
    "            sample_slice = np.zeros(44100)\n",
    "            sample_slice[0:sample.size] = sample\n",
    "            label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)\n",
    "            \n",
    "        for i in range(0, sample.size - 44100, 44100):\n",
    "            sample_slice = sample[i : i + 44100]\n",
    "            label = 0\n",
    "            sample_slice_iteration += 1\n",
    "            if np.max(abs(sample_slice)) < 0.25:\n",
    "                label = 0\n",
    "\n",
    "            samples.append(sample_slice)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))\n",
    "i=450\n",
    "samp=samples[i]\n",
    "sr=2050\n",
    "print(np.max(abs(samp)))\n",
    "print(labels[i])\n",
    "ipd.Audio(samp, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"samples.npy\", samples)\n",
    "np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load(\"samples.npy\")\n",
    "labels = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(wav):\n",
    "    start_ = int(np.random.uniform(-4800,4800))\n",
    "    if start_ >= 0:\n",
    "        wav_time_shift = np.r_[wav[start_:], np.random.uniform(-0.001,0.001, start_)]\n",
    "    else:\n",
    "        wav_time_shift = np.r_[np.random.uniform(-0.001,0.001, -start_), wav[:start_]]\n",
    "    return wav_time_shift\n",
    "\n",
    "def speed_change(wav):\n",
    "    speed_rate = np.random.uniform(0.7,1.3)\n",
    "    wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "    #print('speed rate: %.3f' % speed_rate, '(lower is faster)')\n",
    "    if len(wav_speed_tune) < len(wav):\n",
    "        pad_len = len(wav) - len(wav_speed_tune)\n",
    "        wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len/2)),\n",
    "                               wav_speed_tune,\n",
    "                               np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2)))]\n",
    "    else: \n",
    "        cut_len = len(wav_speed_tune) - len(wav)\n",
    "        wav_speed_tune = wav_speed_tune[int(cut_len/2):int(cut_len/2)+len(wav)]\n",
    "    return wav_speed_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_labels = np.zeros((labels.shape[0]*3,))\n",
    "aug_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples = np.zeros((samples.shape[0]*3, samples.shape[1]))\n",
    "aug_labels = np.zeros((labels.shape[0]*3,))\n",
    "j = 0\n",
    "for i in range (0, len(aug_samples), 3):\n",
    "    aug_samples[i,:] = samples[j,:]\n",
    "    aug_samples[i+1,:] = time_shift(samples[j,:])\n",
    "    aug_samples[i+2,:] = speed_change(samples[j,:])\n",
    "    \n",
    "    aug_labels[i] = labels[j]\n",
    "    aug_labels[i+1] = labels[j]\n",
    "    aug_labels[i+2] = labels[j]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"aug_samples.npy\", aug_samples)\n",
    "np.save(\"aug_labels.npy\", aug_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "samples = np.load(\"samples.npy\")\n",
    "labels = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 1:\n",
    "        l.append('gunshot')\n",
    "    else:\n",
    "        l.append('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.hstack((labels, 1 - labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 6374 6375 6377] TEST: [   5    6    8 ... 6366 6373 6376]\n",
      "TRAIN: [   1    4    5 ... 6375 6376 6377] TEST: [   0    2    3 ... 6370 6371 6372]\n",
      "TRAIN: [   0    2    3 ... 6372 6373 6376] TEST: [   1    4   10 ... 6374 6375 6377]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "'''samples = aug_samples #np.array(samples)\n",
    "labels = aug_labels #np.array(labels)'''\n",
    "\n",
    "#labels = keras.utils.to_categorical(labels, 2)\n",
    "for train_index, test_index in kf.split(samples):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_wav, test_wav = samples[train_index], samples[test_index]\n",
    "    train_label, test_label = labels[train_index], labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveplot(data, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.001\n",
    "generations = 20000\n",
    "num_gens_to_wait = 250\n",
    "batch_size = 32\n",
    "drop_out_rate = 0.2\n",
    "input_shape = (44100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Conv1D add Channel\n",
    "train_wav = np.array(train_wav)\n",
    "test_wav = np.array(test_wav)\n",
    "train_wav = train_wav.reshape(-1,44100,1)\n",
    "test_wav = test_wav.reshape(-1,44100,1)\n",
    "#train_label = keras.utils.to_categorical(train_label, 2)\n",
    "#test_label = keras.utils.to_categorical(test_label, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC metric used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hosle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hosle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\hosle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hosle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=input_shape)\n",
    "nclass = 2\n",
    "\n",
    "x = layers.Convolution1D(16, 9, activation=\"relu\", padding=\"same\")(input_tensor)\n",
    "x = layers.Convolution1D(16, 9, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPool1D(16)(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "x = layers.Convolution1D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Convolution1D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPool1D(8)(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "x = layers.Convolution1D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Convolution1D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(1028, activation=\"relu\")(x)\n",
    "output_tensor = layers.Dense(nclass, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "opt = optimizers.Adam(0.001, 0.001 / 100)\n",
    "\n",
    "model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=[auc, 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 44100, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 44100, 16)         160       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 44100, 16)         2320      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2756, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2756, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2756, 32)          1568      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2756, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 344, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 344, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 344, 128)          12416     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 344, 128)          49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1028)              66820     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2058      \n",
      "=================================================================\n",
      "Total params: 145,982\n",
      "Trainable params: 145,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = '1Dcnngunglass.pkl' \n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc',\n",
    "                  patience=10,\n",
    "                  verbose=1,\n",
    "                  mode='max'),\n",
    "    \n",
    "    ModelCheckpoint(model_filename, monitor='val_acc',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='max'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4252 samples, validate on 2126 samples\n",
      "WARNING:tensorflow:From C:\\Users\\hosle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.2054 - auc: 0.9314 - acc: 0.9285\n",
      "Epoch 00001: val_acc improved from -inf to 0.96566, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 16s 4ms/sample - loss: 0.2043 - auc: 0.9317 - acc: 0.9290 - val_loss: 0.1162 - val_auc: 0.9786 - val_acc: 0.9657\n",
      "Epoch 2/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.1024 - auc: 0.9845 - acc: 0.9524\n",
      "Epoch 00002: val_acc did not improve from 0.96566\n",
      "4252/4252 [==============================] - 12s 3ms/sample - loss: 0.1024 - auc: 0.9845 - acc: 0.9525 - val_loss: 0.1084 - val_auc: 0.9871 - val_acc: 0.9619\n",
      "Epoch 3/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0885 - auc: 0.9884 - acc: 0.9607\n",
      "Epoch 00003: val_acc improved from 0.96566 to 0.96849, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 12s 3ms/sample - loss: 0.0881 - auc: 0.9884 - acc: 0.9607 - val_loss: 0.0786 - val_auc: 0.9899 - val_acc: 0.9685\n",
      "Epoch 4/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0733 - auc: 0.9908 - acc: 0.9725\n",
      "Epoch 00004: val_acc did not improve from 0.96849\n",
      "4252/4252 [==============================] - 12s 3ms/sample - loss: 0.0734 - auc: 0.9908 - acc: 0.9725 - val_loss: 0.1257 - val_auc: 0.9913 - val_acc: 0.9563\n",
      "Epoch 5/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0578 - auc: 0.9918 - acc: 0.9777\n",
      "Epoch 00005: val_acc improved from 0.96849 to 0.97742, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 12s 3ms/sample - loss: 0.0577 - auc: 0.9918 - acc: 0.9777 - val_loss: 0.0623 - val_auc: 0.9926 - val_acc: 0.9774\n",
      "Epoch 6/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0599 - auc: 0.9930 - acc: 0.9794\n",
      "Epoch 00006: val_acc improved from 0.97742 to 0.98354, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0603 - auc: 0.9930 - acc: 0.9791 - val_loss: 0.0674 - val_auc: 0.9934 - val_acc: 0.9835\n",
      "Epoch 7/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0405 - auc: 0.9938 - acc: 0.9867\n",
      "Epoch 00007: val_acc did not improve from 0.98354\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0406 - auc: 0.9938 - acc: 0.9866 - val_loss: 0.0459 - val_auc: 0.9943 - val_acc: 0.9826\n",
      "Epoch 8/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0494 - auc: 0.9946 - acc: 0.9822\n",
      "Epoch 00008: val_acc did not improve from 0.98354\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0494 - auc: 0.9946 - acc: 0.9821 - val_loss: 0.0856 - val_auc: 0.9947 - val_acc: 0.9774\n",
      "Epoch 9/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0444 - auc: 0.9948 - acc: 0.9851\n",
      "Epoch 00009: val_acc improved from 0.98354 to 0.98495, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0441 - auc: 0.9948 - acc: 0.9852 - val_loss: 0.0521 - val_auc: 0.9950 - val_acc: 0.9849\n",
      "Epoch 10/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0313 - auc: 0.9952 - acc: 0.9886\n",
      "Epoch 00010: val_acc did not improve from 0.98495\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0318 - auc: 0.9952 - acc: 0.9882 - val_loss: 0.0577 - val_auc: 0.9954 - val_acc: 0.9826\n",
      "Epoch 11/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0285 - auc: 0.9956 - acc: 0.9901\n",
      "Epoch 00011: val_acc did not improve from 0.98495\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0283 - auc: 0.9956 - acc: 0.9901 - val_loss: 0.1113 - val_auc: 0.9956 - val_acc: 0.9732\n",
      "Epoch 12/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0284 - auc: 0.9957 - acc: 0.9886\n",
      "Epoch 00012: val_acc did not improve from 0.98495\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0283 - auc: 0.9957 - acc: 0.9887 - val_loss: 0.0490 - val_auc: 0.9958 - val_acc: 0.9835\n",
      "Epoch 13/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0340 - auc: 0.9959 - acc: 0.9898\n",
      "Epoch 00013: val_acc improved from 0.98495 to 0.98730, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0338 - auc: 0.9959 - acc: 0.9899 - val_loss: 0.0399 - val_auc: 0.9960 - val_acc: 0.9873\n",
      "Epoch 14/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0229 - auc: 0.9962 - acc: 0.9905\n",
      "Epoch 00014: val_acc did not improve from 0.98730\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0228 - auc: 0.9962 - acc: 0.9906 - val_loss: 0.0501 - val_auc: 0.9963 - val_acc: 0.9859\n",
      "Epoch 15/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0262 - auc: 0.9964 - acc: 0.9905\n",
      "Epoch 00015: val_acc did not improve from 0.98730\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0261 - auc: 0.9964 - acc: 0.9906 - val_loss: 0.0520 - val_auc: 0.9964 - val_acc: 0.9826\n",
      "Epoch 16/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0242 - auc: 0.9965 - acc: 0.9929\n",
      "Epoch 00016: val_acc did not improve from 0.98730\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0241 - auc: 0.9965 - acc: 0.9929 - val_loss: 0.0364 - val_auc: 0.9966 - val_acc: 0.9864\n",
      "Epoch 17/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0209 - auc: 0.9967 - acc: 0.9920\n",
      "Epoch 00017: val_acc did not improve from 0.98730\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0208 - auc: 0.9967 - acc: 0.9920 - val_loss: 0.0554 - val_auc: 0.9968 - val_acc: 0.9831\n",
      "Epoch 18/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0167 - auc: 0.9968 - acc: 0.9950\n",
      "Epoch 00018: val_acc improved from 0.98730 to 0.98871, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0169 - auc: 0.9968 - acc: 0.9948 - val_loss: 0.0409 - val_auc: 0.9969 - val_acc: 0.9887\n",
      "Epoch 19/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0160 - auc: 0.9970 - acc: 0.9938\n",
      "Epoch 00019: val_acc did not improve from 0.98871\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0174 - auc: 0.9970 - acc: 0.9937 - val_loss: 0.0586 - val_auc: 0.9970 - val_acc: 0.9817\n",
      "Epoch 20/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0331 - auc: 0.9971 - acc: 0.9901\n",
      "Epoch 00020: val_acc did not improve from 0.98871\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0330 - auc: 0.9971 - acc: 0.9901 - val_loss: 0.0701 - val_auc: 0.9971 - val_acc: 0.9802\n",
      "Epoch 21/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0243 - auc: 0.9971 - acc: 0.9924\n",
      "Epoch 00021: val_acc did not improve from 0.98871\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0242 - auc: 0.9971 - acc: 0.9925 - val_loss: 0.0419 - val_auc: 0.9972 - val_acc: 0.9878\n",
      "Epoch 22/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0170 - auc: 0.9972 - acc: 0.9948\n",
      "Epoch 00022: val_acc improved from 0.98871 to 0.98918, saving model to 1Dcnngunglass.pkl\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0170 - auc: 0.9972 - acc: 0.9948 - val_loss: 0.0496 - val_auc: 0.9973 - val_acc: 0.9892\n",
      "Epoch 23/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0178 - auc: 0.9973 - acc: 0.9934\n",
      "Epoch 00023: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0177 - auc: 0.9973 - acc: 0.9934 - val_loss: 0.0462 - val_auc: 0.9974 - val_acc: 0.9868\n",
      "Epoch 24/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0127 - auc: 0.9974 - acc: 0.9969\n",
      "Epoch 00024: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0127 - auc: 0.9974 - acc: 0.9969 - val_loss: 0.0437 - val_auc: 0.9974 - val_acc: 0.9873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0108 - auc: 0.9975 - acc: 0.9967\n",
      "Epoch 00025: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0108 - auc: 0.9975 - acc: 0.9967 - val_loss: 0.0670 - val_auc: 0.9975 - val_acc: 0.9882\n",
      "Epoch 26/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0126 - auc: 0.9975 - acc: 0.9960\n",
      "Epoch 00026: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0126 - auc: 0.9975 - acc: 0.9960 - val_loss: 0.0536 - val_auc: 0.9975 - val_acc: 0.9864\n",
      "Epoch 27/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0136 - auc: 0.9975 - acc: 0.9957\n",
      "Epoch 00027: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0135 - auc: 0.9975 - acc: 0.9958 - val_loss: 0.0557 - val_auc: 0.9976 - val_acc: 0.9849\n",
      "Epoch 28/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0108 - auc: 0.9976 - acc: 0.9953\n",
      "Epoch 00028: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0107 - auc: 0.9976 - acc: 0.9953 - val_loss: 0.0531 - val_auc: 0.9976 - val_acc: 0.9873\n",
      "Epoch 29/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0084 - auc: 0.9976 - acc: 0.9967\n",
      "Epoch 00029: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0083 - auc: 0.9976 - acc: 0.9967 - val_loss: 0.0543 - val_auc: 0.9977 - val_acc: 0.9868\n",
      "Epoch 30/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0125 - auc: 0.9977 - acc: 0.9962\n",
      "Epoch 00030: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0125 - auc: 0.9977 - acc: 0.9962 - val_loss: 0.0481 - val_auc: 0.9977 - val_acc: 0.9868\n",
      "Epoch 31/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0188 - auc: 0.9977 - acc: 0.9948\n",
      "Epoch 00031: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0190 - auc: 0.9977 - acc: 0.9946 - val_loss: 0.0576 - val_auc: 0.9977 - val_acc: 0.9859\n",
      "Epoch 32/100\n",
      "4224/4252 [============================>.] - ETA: 0s - loss: 0.0064 - auc: 0.9977 - acc: 0.9981\n",
      "Epoch 00032: val_acc did not improve from 0.98918\n",
      "4252/4252 [==============================] - 13s 3ms/sample - loss: 0.0064 - auc: 0.9977 - acc: 0.9981 - val_loss: 0.0884 - val_auc: 0.9977 - val_acc: 0.9802\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ba42b467f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_wav, train_label, \n",
    "          validation_data=[test_wav, test_label],\n",
    "          batch_size=batch_size,\n",
    "          callbacks = callbacks,\n",
    "          epochs=100,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"1Dcnngunglass.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"C:\\\\Users\\\\hosle\\\\Downloads\\\\Metal Bang-SoundBible.com-672025076.wav\"\n",
    "#mypath = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\extra\\\\320134.wav\"\n",
    "#mypath = \"C:\\\\Users\\\\hosle\\\\Documents\\\\_REU2019\\\\_project\\\\gunshot_detection\\\\fireworks\\\\56608__syna-max__fireworks-well-i-guess-you-missed-it.wav\"\n",
    "\n",
    "fire, sr = librosa.load(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = []\n",
    "if (fire.size <= 44100):\n",
    "    sample_slice = np.zeros(44100)\n",
    "    sample_slice[0:fire.size] = fire\n",
    "    validation.append(sample_slice)\n",
    "\n",
    "for i in range(0, fire.size - 44100, 44100):\n",
    "    sample_slice = fire[i : i + 44100]\n",
    "    validation.append(sample_slice)\n",
    "    \n",
    "validation = np.array(validation)\n",
    "validation_1 = validation.reshape(-1,44100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(validation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred1 = lb.inverse_transform(val_pred[:, 0])\n",
    "print(len(val_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(val_pred1 == 'gunshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0, 1] = gunshot\n",
    "i = 0\n",
    "print(val_pred[i, :])\n",
    "print(val_pred1[i])\n",
    "show(validation [i])\n",
    "ipd.Audio(validation [i], rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tflite converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gunshot_sound_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_keras_model_file() got an unexpected keyword argument 'allow_custom_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9e7e6aad9063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_custom_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# custom_objects={'auc': auc})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n made the converter using from_keras_model \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: from_keras_model_file() got an unexpected keyword argument 'allow_custom_ops'"
     ]
    }
   ],
   "source": [
    "converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(model_name, allow_custom_ops = True)# custom_objects={'auc': auc})\n",
    "print(\"\\n made the converter using from_keras_model \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
