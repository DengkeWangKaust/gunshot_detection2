{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob \n",
    "import numpy as np\n",
    "import sys\n",
    "import keras\n",
    "import cv2\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(spectrogram):\n",
    "    plt.imshow(spectrogram)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image shape of the generated spectrograms\n",
    "input_size = (224, 230)\n",
    "\n",
    "def get_spec(files):\n",
    "    path = 'path to files'\n",
    "    os.chdir(path)\n",
    "            \n",
    "    N = len(files)\n",
    "    data = np.zeros((N, input_size[0], input_size[1], 3))\n",
    "\n",
    "    for i in range(0, len(files), 1):\n",
    "        img = cv2.imread(files[i])\n",
    "        data[i] = img\n",
    "            \n",
    "    return data\n",
    "\n",
    "def shift_img(img, M):\n",
    "    img = cv2.imread(files[0])\n",
    "    rows, cols, _ = img.shape\n",
    "\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return dst\n",
    "\n",
    "def fit_y(y):\n",
    "    newy = []\n",
    "    for i in range(len(y)):\n",
    "        newy.append(y[i])\n",
    "        newy.append(y[i])\n",
    "        newy.append(y[i])\n",
    "        newy.append(y[i])\n",
    "    return np.array(newy)\n",
    "\n",
    "def time_shift(wav):\n",
    "    start_ = int(np.random.uniform(-4800,4800))\n",
    "    if start_ >= 0:\n",
    "        wav_time_shift = np.r_[wav[start_:], np.random.uniform(-0.001,0.001, start_)]\n",
    "    else:\n",
    "        wav_time_shift = np.r_[np.random.uniform(-0.001,0.001, -start_), wav[:start_]]\n",
    "    return wav_time_shift\n",
    "\n",
    "def speed_change(wav):\n",
    "    speed_rate = np.random.uniform(0.7,1.3)\n",
    "    wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "    #print('speed rate: %.3f' % speed_rate, '(lower is faster)')\n",
    "    if len(wav_speed_tune) < len(wav):\n",
    "        pad_len = len(wav) - len(wav_speed_tune)\n",
    "        wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len/2)),\n",
    "                               wav_speed_tune,\n",
    "                               np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2)))]\n",
    "    else: \n",
    "        cut_len = len(wav_speed_tune) - len(wav)\n",
    "        wav_speed_tune = wav_speed_tune[int(cut_len/2):int(cut_len/2)+len(wav)]\n",
    "    return wav_speed_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN MODE NOW\n",
    "def create_spectrogram(filename,name):\n",
    "    plt.interactive(False)\n",
    "    clip1, sample_rate = librosa.load(filename, sr=None)\n",
    "    clip2 = time_shift(clip1)\n",
    "    clip3 = speed_change(clip1)\n",
    "    clip4 = speed_change(clip2)\n",
    "    save_spec(clip1, sample_rate, name + \"_0\")\n",
    "    save_spec(clip2, sample_rate, name + \"_1\")\n",
    "    save_spec(clip3, sample_rate, name + \"_2\")\n",
    "    save_spec(clip4, sample_rate, name + \"_3\")\n",
    "    \n",
    "def save_spec(clip, sample_rate, name):\n",
    "    plt.interactive(False)\n",
    "    \n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    filename  = 'path to new folder' + name + '.jpg'\n",
    "    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    \n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    del filename,name,clip,sample_rate,fig,ax,S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train spectogram creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create spectrograms from the wav files. Augmentation multiplies the file count by 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path to urban wav files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d9f26be25aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmypath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"path to urban wav files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path to urban wav files'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "mypath = \"path to urban wav files\"\n",
    "dr = listdir(mypath)\n",
    "for i in range(len(dr)):\n",
    "    dr[i] = int(dr[i][:-4])\n",
    "dr = np.sort(dr)\n",
    "files = np.zeros(len(dr)).astype('str')\n",
    "for i in range(len(dr)):\n",
    "    files[i] = str(dr[i]) + '.wav'\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('project directory')\n",
    "y = pd.read_csv('train.csv')\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('path to wave files')\n",
    "for i in range(len(files)):\n",
    "    create_spectrogram(files[i], str(y[i, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"path to folder with spectrograms\"\n",
    "dr = listdir(mypath)\n",
    "for i in range(len(dr)):\n",
    "    dr[i] = int(dr[i][:-6])\n",
    "dr = np.sort(dr)\n",
    "files = np.zeros(len(dr)).astype('str')\n",
    "for i in range(0,len(dr),4):\n",
    "    files[i] = str(dr[i]) + '_0.jpg'\n",
    "    files[i+1] = str(dr[i+1]) + '_1.jpg'\n",
    "    files[i+2] = str(dr[i+2]) + '_2.jpg'\n",
    "    files[i+3] = str(dr[i+3]) + '_3.jpg'\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_spec(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.nbytes / (2**10)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('project directory')\n",
    "y_id = pd.read_csv(\"train.csv\")\n",
    "y_id = y_id.values[:, 1].astype('<U16')\n",
    "y_id = fit_y(y_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('project directory')\n",
    "channel = 3\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y_id)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.33, random_state = 42)\n",
    "X_train = X_train.astype ('float32')\n",
    "X_test = X_test.astype ('float32')\n",
    "X_val = X_val.astype ('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "\n",
    "X_train = X_train.reshape (-1, input_size[0], input_size[1], channel)\n",
    "X_test = X_test.reshape (-1, input_size[0], input_size[1], channel)\n",
    "X_val = X_val.reshape (-1, input_size[0], input_size[1], channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(X_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size = 40, dim = input_size, n_channels = 3,\n",
    "                 n_classes = 10, shuffle = True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        y = self.labels[indexes]\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            image = cv2.imread('path to spectrograms' + ID)\n",
    "            #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            #X[i,] = gray.reshape((224,230,1))\n",
    "            X[i,] = image\n",
    "            X[i,] /= 255\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "n_labels = 10\n",
    "model = Sequential()\n",
    "chanDim = -1\n",
    "channel = 3\n",
    "#channel = 1 for grayscale\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape = (input_size[0], input_size[1], channel)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(200, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(Conv2D(200, (3, 3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis = chanDim))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "# Step 3 - Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_labels))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Compiling the CNN\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "opt = Adam(lr = INIT_LR, decay = INIT_LR / EPOCHS)\n",
    "#opt = SGD(lr = 0.01, decay = 1e-8, momentum = 0.9, nesterov=True)\n",
    "model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('project directory')\n",
    "y_csv = pd.read_csv(\"train.csv\")\n",
    "l = y_csv.values[:, 1].astype('<U16')\n",
    "l = fit_y(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(l)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "mypath = \"path to spectrogram files\"\n",
    "dr = listdir(mypath)\n",
    "for i in range(len(dr)):\n",
    "    dr[i] = int(dr[i][:-6])\n",
    "dr = np.sort(dr)\n",
    "files = np.zeros(len(dr)).astype('str')\n",
    "for i in range(0,len(dr),4):\n",
    "    files[i] = str(dr[i]) + '_0.jpg'\n",
    "    files[i+1] = str(dr[i+1]) + '_1.jpg'\n",
    "    files[i+2] = str(dr[i+2]) + '_2.jpg'\n",
    "    files[i+3] = str(dr[i+3]) + '_3.jpg'\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_id = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id, test_id, train_label, test_label = train_test_split(y_id, y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(train_id, train_label)\n",
    "validation_generator = DataGenerator(test_id, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'cnnmodel.pkl' \n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc',\n",
    "                  patience=10,\n",
    "                  verbose=1,\n",
    "                  mode='auto'),\n",
    "    \n",
    "    ModelCheckpoint(model_filename, monitor='val_acc',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='auto'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "History = model.fit_generator(generator = training_generator,\n",
    "                                validation_data = validation_generator,\n",
    "                                epochs = 50,\n",
    "                                callbacks = callbacks,\n",
    "                                verbose = 1,\n",
    "                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(X_test))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(History.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(History.history['acc'])\n",
    "plt.plot(History.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
