{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File Directory \n",
    "import glob\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "\n",
    "# Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "# Data Pre-processing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import librosa\n",
    "import soundfile\n",
    "import re\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
    "from keras import losses, models, optimizers\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "py.init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[]\n",
    "labels = []\n",
    "sampling_rate_per_two_seconds = 44100\n",
    "input_shape = (sampling_rate_per_two_seconds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from the files as numpy arrays\n",
    "samples = np.load(\"/home/lauogden/data/gunshot_sound_samples.npy\")\n",
    "labels = np.load(\"/home/lauogden/data/gunshot_sound_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 2445 2446 2447] TEST: [   8   11   15   18   20   21   22   23   27   33   36   37   39   43\n",
      "   55   56   58   65   68   69   70   74   77   79   81   88   90   92\n",
      "   93   94   98   99  103  105  106  107  108  114  115  119  120  121\n",
      "  128  132  133  137  139  140  147  149  151  153  154  155  156  159\n",
      "  160  161  162  163  173  180  186  192  194  198  202  203  210  211\n",
      "  213  215  217  219  227  230  237  240  243  248  250  251  252  256\n",
      "  263  264  266  267  270  272  274  278  279  280  281  289  290  293\n",
      "  303  307  309  313  315  316  317  318  324  325  327  331  332  333\n",
      "  337  338  341  342  347  352  353  359  364  365  368  369  372  380\n",
      "  381  383  385  387  400  402  403  404  407  408  410  411  412  425\n",
      "  427  429  435  436  438  439  444  447  448  450  451  455  461  465\n",
      "  469  473  475  476  480  484  487  492  496  497  500  501  513  515\n",
      "  518  519  521  524  529  541  543  548  550  551  552  555  561  566\n",
      "  567  571  574  575  577  584  585  586  598  603  605  606  610  612\n",
      "  615  618  619  621  627  628  634  639  642  644  648  655  659  666\n",
      "  668  670  672  673  675  676  682  685  686  687  692  693  694  700\n",
      "  701  702  709  710  715  720  721  724  725  731  734  736  742  746\n",
      "  748  751  752  753  754  759  760  761  765  771  773  774  777  780\n",
      "  787  790  792  793  795  796  801  804  807  808  810  815  818  819\n",
      "  822  823  824  829  830  831  833  834  835  838  842  845  846  850\n",
      "  852  853  858  859  861  863  864  865  866  868  869  881  891  892\n",
      "  900  901  910  916  924  928  937  944  947  950  954  956  957  959\n",
      "  960  961  963  965  970  976  977  978  979  982  983  992  994  995\n",
      "  999 1002 1005 1008 1011 1014 1022 1023 1024 1026 1030 1032 1035 1039\n",
      " 1041 1042 1044 1045 1049 1052 1053 1055 1057 1060 1068 1069 1070 1075\n",
      " 1076 1077 1082 1086 1089 1097 1098 1101 1103 1105 1107 1108 1110 1111\n",
      " 1113 1117 1119 1127 1128 1131 1132 1133 1137 1138 1139 1142 1147 1148\n",
      " 1150 1156 1162 1164 1170 1174 1175 1182 1184 1187 1198 1202 1205 1207\n",
      " 1208 1214 1215 1217 1218 1222 1225 1229 1234 1235 1236 1239 1240 1241\n",
      " 1243 1244 1246 1259 1260 1263 1268 1271 1274 1280 1282 1283 1284 1288\n",
      " 1289 1292 1296 1298 1300 1301 1302 1312 1316 1319 1320 1323 1324 1326\n",
      " 1334 1336 1341 1343 1346 1350 1352 1358 1363 1364 1369 1371 1372 1376\n",
      " 1379 1380 1381 1382 1384 1386 1389 1393 1394 1397 1398 1402 1403 1409\n",
      " 1415 1417 1420 1421 1423 1429 1431 1433 1434 1435 1440 1441 1446 1447\n",
      " 1448 1449 1450 1451 1454 1461 1463 1464 1466 1473 1479 1482 1485 1487\n",
      " 1488 1496 1497 1498 1500 1505 1508 1510 1513 1515 1518 1521 1522 1524\n",
      " 1529 1530 1534 1537 1539 1540 1541 1553 1555 1557 1558 1559 1562 1564\n",
      " 1573 1580 1583 1584 1585 1588 1589 1590 1594 1597 1600 1602 1605 1608\n",
      " 1609 1610 1612 1616 1619 1625 1626 1629 1633 1637 1642 1648 1651 1652\n",
      " 1654 1656 1657 1666 1667 1668 1670 1671 1672 1678 1680 1681 1683 1684\n",
      " 1685 1686 1688 1692 1697 1699 1703 1709 1711 1715 1716 1717 1720 1721\n",
      " 1722 1723 1724 1726 1731 1732 1739 1740 1741 1745 1749 1750 1752 1754\n",
      " 1760 1761 1762 1763 1764 1766 1767 1768 1773 1775 1776 1781 1782 1785\n",
      " 1788 1794 1797 1800 1801 1805 1811 1812 1813 1815 1817 1821 1826 1827\n",
      " 1845 1847 1852 1859 1862 1864 1867 1872 1874 1876 1877 1881 1882 1884\n",
      " 1886 1888 1898 1899 1903 1906 1907 1910 1911 1917 1921 1925 1928 1929\n",
      " 1936 1938 1939 1941 1944 1947 1951 1952 1954 1957 1962 1963 1967 1968\n",
      " 1969 1970 1978 1982 1983 1984 1987 1988 1991 1992 1993 1995 1999 2000\n",
      " 2001 2003 2007 2008 2012 2013 2014 2016 2019 2021 2022 2026 2028 2030\n",
      " 2035 2046 2048 2051 2055 2057 2059 2065 2067 2073 2077 2078 2084 2091\n",
      " 2096 2099 2101 2103 2106 2116 2120 2124 2125 2126 2130 2131 2137 2143\n",
      " 2144 2147 2148 2149 2153 2157 2162 2163 2165 2168 2169 2179 2182 2184\n",
      " 2193 2198 2201 2206 2208 2217 2218 2220 2226 2228 2231 2232 2237 2238\n",
      " 2240 2241 2242 2244 2245 2249 2252 2254 2255 2259 2261 2263 2264 2266\n",
      " 2268 2269 2272 2273 2276 2277 2283 2290 2294 2300 2301 2302 2304 2305\n",
      " 2309 2314 2317 2318 2319 2321 2322 2323 2328 2333 2338 2339 2341 2345\n",
      " 2349 2357 2358 2367 2374 2375 2379 2383 2385 2387 2388 2390 2393 2395\n",
      " 2399 2405 2406 2410 2411 2413 2416 2417 2418 2420 2424 2430 2432 2433\n",
      " 2435 2437 2438 2440]\n",
      "TRAIN: [   2    3    5 ... 2442 2444 2445] TEST: [   0    1    4   12   13   17   24   25   26   28   31   32   35   44\n",
      "   46   48   53   54   57   59   60   61   62   82   83   84   86   91\n",
      "   97  109  111  113  122  123  124  126  136  138  142  145  146  148\n",
      "  150  152  167  168  169  175  177  178  181  182  184  185  187  191\n",
      "  195  199  204  205  209  212  214  218  226  228  232  233  234  235\n",
      "  236  241  242  245  246  255  259  260  261  262  265  269  271  275\n",
      "  276  277  284  285  286  287  291  295  298  300  302  304  305  306\n",
      "  310  311  319  320  322  323  326  328  330  339  343  344  346  348\n",
      "  350  355  358  360  362  363  366  367  370  373  374  375  376  377\n",
      "  379  382  384  386  390  391  396  405  409  414  416  421  424  430\n",
      "  432  433  437  442  443  445  446  449  452  453  458  459  463  466\n",
      "  470  471  472  474  477  478  490  491  493  494  495  504  505  512\n",
      "  520  525  527  528  531  532  533  535  536  538  539  542  544  545\n",
      "  557  559  560  563  569  572  576  578  579  580  581  582  583  588\n",
      "  589  599  600  607  608  611  617  620  622  630  632  633  637  638\n",
      "  640  641  643  645  646  647  649  651  654  657  661  662  663  664\n",
      "  665  667  669  674  679  683  688  690  691  698  699  704  707  708\n",
      "  711  713  714  716  719  722  723  726  727  728  739  740  741  747\n",
      "  749  757  758  764  768  770  775  779  784  786  789  791  798  799\n",
      "  814  820  827  828  839  841  844  848  856  862  867  870  871  873\n",
      "  874  875  878  883  884  885  887  895  896  897  898  899  903  904\n",
      "  906  909  911  913  914  918  920  922  925  926  930  931  933  934\n",
      "  935  936  938  939  941  943  945  946  948  949  951  952  953  955\n",
      "  958  962  964  967  968  975  984  985  986  987  989  990  993  998\n",
      " 1009 1012 1013 1015 1018 1019 1020 1028 1034 1037 1038 1040 1046 1050\n",
      " 1054 1056 1062 1063 1064 1066 1067 1071 1072 1073 1074 1081 1083 1084\n",
      " 1085 1087 1088 1090 1091 1099 1100 1102 1104 1109 1114 1115 1116 1121\n",
      " 1122 1124 1129 1130 1135 1140 1144 1146 1149 1152 1158 1159 1160 1163\n",
      " 1168 1169 1171 1172 1173 1178 1180 1183 1185 1186 1188 1190 1191 1192\n",
      " 1193 1194 1196 1197 1199 1200 1201 1203 1204 1211 1213 1219 1221 1228\n",
      " 1237 1242 1245 1247 1248 1250 1251 1253 1254 1256 1258 1261 1264 1267\n",
      " 1272 1273 1275 1279 1281 1287 1290 1293 1299 1303 1305 1306 1308 1310\n",
      " 1311 1317 1318 1321 1328 1330 1332 1333 1335 1338 1339 1340 1344 1348\n",
      " 1349 1354 1357 1359 1361 1366 1367 1373 1374 1378 1383 1387 1388 1391\n",
      " 1392 1400 1401 1408 1410 1416 1419 1422 1426 1427 1430 1432 1436 1439\n",
      " 1442 1443 1452 1453 1456 1457 1458 1459 1460 1462 1465 1467 1470 1474\n",
      " 1476 1477 1484 1492 1493 1499 1503 1509 1517 1519 1520 1527 1542 1547\n",
      " 1550 1552 1561 1563 1565 1567 1571 1572 1578 1581 1582 1592 1593 1599\n",
      " 1601 1607 1615 1617 1618 1620 1621 1622 1624 1635 1638 1644 1645 1650\n",
      " 1653 1655 1661 1662 1665 1669 1676 1679 1682 1687 1689 1690 1698 1700\n",
      " 1704 1705 1707 1708 1710 1713 1714 1718 1725 1727 1728 1729 1733 1737\n",
      " 1742 1743 1746 1751 1753 1756 1759 1769 1772 1774 1777 1778 1780 1783\n",
      " 1786 1787 1790 1791 1792 1795 1796 1799 1803 1804 1807 1808 1809 1810\n",
      " 1814 1816 1818 1819 1820 1823 1824 1831 1834 1835 1837 1838 1843 1851\n",
      " 1854 1855 1861 1863 1868 1869 1870 1871 1873 1875 1878 1880 1887 1889\n",
      " 1891 1892 1894 1901 1908 1909 1913 1919 1920 1923 1927 1930 1933 1934\n",
      " 1935 1937 1948 1955 1956 1959 1961 1964 1972 1976 1985 1989 1990 1996\n",
      " 1997 1998 2002 2004 2006 2009 2018 2023 2024 2025 2027 2031 2032 2033\n",
      " 2036 2037 2038 2040 2042 2043 2044 2047 2053 2054 2058 2060 2061 2062\n",
      " 2064 2066 2068 2074 2075 2079 2081 2083 2086 2087 2088 2089 2090 2095\n",
      " 2097 2098 2100 2104 2105 2107 2109 2111 2114 2117 2118 2119 2132 2134\n",
      " 2139 2140 2141 2145 2154 2159 2161 2167 2176 2178 2183 2188 2191 2194\n",
      " 2196 2203 2204 2210 2211 2214 2215 2216 2219 2222 2223 2224 2225 2229\n",
      " 2230 2233 2236 2247 2250 2251 2253 2256 2260 2262 2265 2270 2271 2274\n",
      " 2275 2278 2282 2284 2285 2287 2291 2293 2295 2296 2297 2298 2303 2306\n",
      " 2316 2326 2329 2332 2335 2337 2347 2350 2351 2352 2355 2356 2359 2360\n",
      " 2363 2365 2368 2370 2371 2372 2373 2377 2380 2381 2382 2389 2392 2396\n",
      " 2398 2400 2401 2404 2412 2414 2415 2421 2422 2423 2425 2426 2428 2434\n",
      " 2439 2443 2446 2447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    4 ... 2443 2446 2447] TEST: [   2    3    5    6    7    9   10   14   16   19   29   30   34   38\n",
      "   40   41   42   45   47   49   50   51   52   63   64   66   67   71\n",
      "   72   73   75   76   78   80   85   87   89   95   96  100  101  102\n",
      "  104  110  112  116  117  118  125  127  129  130  131  134  135  141\n",
      "  143  144  157  158  164  165  166  170  171  172  174  176  179  183\n",
      "  188  189  190  193  196  197  200  201  206  207  208  216  220  221\n",
      "  222  223  224  225  229  231  238  239  244  247  249  253  254  257\n",
      "  258  268  273  282  283  288  292  294  296  297  299  301  308  312\n",
      "  314  321  329  334  335  336  340  345  349  351  354  356  357  361\n",
      "  371  378  388  389  392  393  394  395  397  398  399  401  406  413\n",
      "  415  417  418  419  420  422  423  426  428  431  434  440  441  454\n",
      "  456  457  460  462  464  467  468  479  481  482  483  485  486  488\n",
      "  489  498  499  502  503  506  507  508  509  510  511  514  516  517\n",
      "  522  523  526  530  534  537  540  546  547  549  553  554  556  558\n",
      "  562  564  565  568  570  573  587  590  591  592  593  594  595  596\n",
      "  597  601  602  604  609  613  614  616  623  624  625  626  629  631\n",
      "  635  636  650  652  653  656  658  660  671  677  678  680  681  684\n",
      "  689  695  696  697  703  705  706  712  717  718  729  730  732  733\n",
      "  735  737  738  743  744  745  750  755  756  762  763  766  767  769\n",
      "  772  776  778  781  782  783  785  788  794  797  800  802  803  805\n",
      "  806  809  811  812  813  816  817  821  825  826  832  836  837  840\n",
      "  843  847  849  851  854  855  857  860  872  876  877  879  880  882\n",
      "  886  888  889  890  893  894  902  905  907  908  912  915  917  919\n",
      "  921  923  927  929  932  940  942  966  969  971  972  973  974  980\n",
      "  981  988  991  996  997 1000 1001 1003 1004 1006 1007 1010 1016 1017\n",
      " 1021 1025 1027 1029 1031 1033 1036 1043 1047 1048 1051 1058 1059 1061\n",
      " 1065 1078 1079 1080 1092 1093 1094 1095 1096 1106 1112 1118 1120 1123\n",
      " 1125 1126 1134 1136 1141 1143 1145 1151 1153 1154 1155 1157 1161 1165\n",
      " 1166 1167 1176 1177 1179 1181 1189 1195 1206 1209 1210 1212 1216 1220\n",
      " 1223 1224 1226 1227 1230 1231 1232 1233 1238 1249 1252 1255 1257 1262\n",
      " 1265 1266 1269 1270 1276 1277 1278 1285 1286 1291 1294 1295 1297 1304\n",
      " 1307 1309 1313 1314 1315 1322 1325 1327 1329 1331 1337 1342 1345 1347\n",
      " 1351 1353 1355 1356 1360 1362 1365 1368 1370 1375 1377 1385 1390 1395\n",
      " 1396 1399 1404 1405 1406 1407 1411 1412 1413 1414 1418 1424 1425 1428\n",
      " 1437 1438 1444 1445 1455 1468 1469 1471 1472 1475 1478 1480 1481 1483\n",
      " 1486 1489 1490 1491 1494 1495 1501 1502 1504 1506 1507 1511 1512 1514\n",
      " 1516 1523 1525 1526 1528 1531 1532 1533 1535 1536 1538 1543 1544 1545\n",
      " 1546 1548 1549 1551 1554 1556 1560 1566 1568 1569 1570 1574 1575 1576\n",
      " 1577 1579 1586 1587 1591 1595 1596 1598 1603 1604 1606 1611 1613 1614\n",
      " 1623 1627 1628 1630 1631 1632 1634 1636 1639 1640 1641 1643 1646 1647\n",
      " 1649 1658 1659 1660 1663 1664 1673 1674 1675 1677 1691 1693 1694 1695\n",
      " 1696 1701 1702 1706 1712 1719 1730 1734 1735 1736 1738 1744 1747 1748\n",
      " 1755 1757 1758 1765 1770 1771 1779 1784 1789 1793 1798 1802 1806 1822\n",
      " 1825 1828 1829 1830 1832 1833 1836 1839 1840 1841 1842 1844 1846 1848\n",
      " 1849 1850 1853 1856 1857 1858 1860 1865 1866 1879 1883 1885 1890 1893\n",
      " 1895 1896 1897 1900 1902 1904 1905 1912 1914 1915 1916 1918 1922 1924\n",
      " 1926 1931 1932 1940 1942 1943 1945 1946 1949 1950 1953 1958 1960 1965\n",
      " 1966 1971 1973 1974 1975 1977 1979 1980 1981 1986 1994 2005 2010 2011\n",
      " 2015 2017 2020 2029 2034 2039 2041 2045 2049 2050 2052 2056 2063 2069\n",
      " 2070 2071 2072 2076 2080 2082 2085 2092 2093 2094 2102 2108 2110 2112\n",
      " 2113 2115 2121 2122 2123 2127 2128 2129 2133 2135 2136 2138 2142 2146\n",
      " 2150 2151 2152 2155 2156 2158 2160 2164 2166 2170 2171 2172 2173 2174\n",
      " 2175 2177 2180 2181 2185 2186 2187 2189 2190 2192 2195 2197 2199 2200\n",
      " 2202 2205 2207 2209 2212 2213 2221 2227 2234 2235 2239 2243 2246 2248\n",
      " 2257 2258 2267 2279 2280 2281 2286 2288 2289 2292 2299 2307 2308 2310\n",
      " 2311 2312 2313 2315 2320 2324 2325 2327 2330 2331 2334 2336 2340 2342\n",
      " 2343 2344 2346 2348 2353 2354 2361 2362 2364 2366 2369 2376 2378 2384\n",
      " 2386 2391 2394 2397 2402 2403 2407 2408 2409 2419 2427 2429 2431 2436\n",
      " 2441 2442 2444 2445]\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "samples = np.array(samples)\n",
    "labels = np.array(labels)\n",
    "for train_index, test_index in kf.split(samples):\n",
    "    train_wav, test_wav = samples[train_index], samples[test_index]\n",
    "    train_label, test_label = labels[train_index], labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wav = np.array(train_wav)\n",
    "test_wav = np.array(test_wav)\n",
    "\n",
    "#Reshape data\n",
    "train_wav = train_wav.reshape(-1,44100,1)\n",
    "test_wav = test_wav.reshape(-1,44100,1)\n",
    "train_label = keras.utils.to_categorical(train_label, 2)\n",
    "test_label = keras.utils.to_categorical(test_label, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.001\n",
    "generations = 20000\n",
    "num_gens_to_wait = 250\n",
    "batch_size = 32\n",
    "drop_out_rate = 0.2\n",
    "input_shape = (44100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=input_shape)\n",
    "\n",
    "x = layers.Conv1D(8, 11, padding='valid', activation='relu', strides=1)(input_tensor)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(16, 7, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(4)(x)\n",
    "x = layers.Conv1D(32, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(4)(x)\n",
    "x = layers.Conv1D(64, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(6)(x)\n",
    "x = layers.Conv1D(128, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(6)(x)\n",
    "x = layers.Conv1D(256, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(6)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Dense(50, activation='relu')(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Dense(20, activation='relu')(x)\n",
    "output_tensor = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(lr = lr),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = '/home/lauogden/gunshot_cnn_model.pkl' \n",
    "\n",
    "# add callbacks to stop early if it stops improving\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc',\n",
    "                  patience=10,\n",
    "                  verbose=1,\n",
    "                  mode='auto'),\n",
    "    \n",
    "    ModelCheckpoint(model_filename, monitor='val_acc',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='auto'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 44100, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 44090, 8)          96        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 22045, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 22039, 16)         912       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5509, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5505, 32)          2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1376, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1372, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 228, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 226, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 35, 256)           98560     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               128100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 271,401\n",
      "Trainable params: 271,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 579 samples, validate on 289 samples\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "579/579 [==============================] - 4s 7ms/sample - loss: 1.0894 - acc: 0.4957 - val_loss: 1.0343 - val_acc: 0.6747\n",
      "Epoch 2/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 1.0312 - acc: 0.6546 - val_loss: 0.9374 - val_acc: 0.6851\n",
      "Epoch 3/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.9638 - acc: 0.6736 - val_loss: 0.8990 - val_acc: 0.7336\n",
      "Epoch 4/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.8959 - acc: 0.7772 - val_loss: 0.8288 - val_acc: 0.7647\n",
      "Epoch 5/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.8225 - acc: 0.7634 - val_loss: 0.7234 - val_acc: 0.7509\n",
      "Epoch 6/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.7007 - acc: 0.7720 - val_loss: 0.6365 - val_acc: 0.7647\n",
      "Epoch 7/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.6037 - acc: 0.7824 - val_loss: 0.5915 - val_acc: 0.7682\n",
      "Epoch 8/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5679 - acc: 0.7962 - val_loss: 0.5448 - val_acc: 0.7647\n",
      "Epoch 9/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5747 - acc: 0.7737 - val_loss: 0.5117 - val_acc: 0.7612\n",
      "Epoch 10/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5303 - acc: 0.7945 - val_loss: 0.5012 - val_acc: 0.7647\n",
      "Epoch 11/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5583 - acc: 0.7772 - val_loss: 0.4851 - val_acc: 0.7958\n",
      "Epoch 12/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5356 - acc: 0.8083 - val_loss: 0.5257 - val_acc: 0.7751\n",
      "Epoch 13/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5185 - acc: 0.8031 - val_loss: 0.4940 - val_acc: 0.7924\n",
      "Epoch 14/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.5063 - acc: 0.8014 - val_loss: 0.4834 - val_acc: 0.7993\n",
      "Epoch 15/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4824 - acc: 0.8135 - val_loss: 0.5018 - val_acc: 0.7785\n",
      "Epoch 16/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4945 - acc: 0.8169 - val_loss: 0.4955 - val_acc: 0.7924\n",
      "Epoch 17/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4777 - acc: 0.8048 - val_loss: 0.4486 - val_acc: 0.8166\n",
      "Epoch 18/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4554 - acc: 0.8273 - val_loss: 0.4416 - val_acc: 0.8235\n",
      "Epoch 19/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4729 - acc: 0.8342 - val_loss: 0.4690 - val_acc: 0.8028\n",
      "Epoch 20/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4597 - acc: 0.8325 - val_loss: 0.5022 - val_acc: 0.8062\n",
      "Epoch 21/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4510 - acc: 0.8238 - val_loss: 0.4492 - val_acc: 0.8201\n",
      "Epoch 22/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4495 - acc: 0.8290 - val_loss: 0.4461 - val_acc: 0.8235\n",
      "Epoch 23/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4341 - acc: 0.8342 - val_loss: 0.4818 - val_acc: 0.8201\n",
      "Epoch 24/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4277 - acc: 0.8342 - val_loss: 0.4605 - val_acc: 0.8304\n",
      "Epoch 25/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3977 - acc: 0.8446 - val_loss: 0.4538 - val_acc: 0.8374\n",
      "Epoch 26/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4025 - acc: 0.8497 - val_loss: 0.4396 - val_acc: 0.8443\n",
      "Epoch 27/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.4114 - acc: 0.8653 - val_loss: 0.4246 - val_acc: 0.8581\n",
      "Epoch 28/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3906 - acc: 0.8722 - val_loss: 0.4168 - val_acc: 0.8478\n",
      "Epoch 29/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3943 - acc: 0.8774 - val_loss: 0.4167 - val_acc: 0.8478\n",
      "Epoch 30/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3626 - acc: 0.8895 - val_loss: 0.4611 - val_acc: 0.8339\n",
      "Epoch 31/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3684 - acc: 0.8860 - val_loss: 0.4846 - val_acc: 0.8512\n",
      "Epoch 32/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3649 - acc: 0.8877 - val_loss: 0.4225 - val_acc: 0.8685\n",
      "Epoch 33/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3506 - acc: 0.8998 - val_loss: 0.3994 - val_acc: 0.8824\n",
      "Epoch 34/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3349 - acc: 0.9016 - val_loss: 0.3974 - val_acc: 0.8858\n",
      "Epoch 35/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3373 - acc: 0.9085 - val_loss: 0.4138 - val_acc: 0.8720\n",
      "Epoch 36/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3207 - acc: 0.9016 - val_loss: 0.4431 - val_acc: 0.8720\n",
      "Epoch 37/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3134 - acc: 0.9171 - val_loss: 0.4315 - val_acc: 0.8824\n",
      "Epoch 38/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2929 - acc: 0.9188 - val_loss: 0.4306 - val_acc: 0.8962\n",
      "Epoch 39/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3066 - acc: 0.9119 - val_loss: 0.4388 - val_acc: 0.8824\n",
      "Epoch 40/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2942 - acc: 0.9136 - val_loss: 0.4257 - val_acc: 0.8789\n",
      "Epoch 41/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2951 - acc: 0.9171 - val_loss: 0.4688 - val_acc: 0.8858\n",
      "Epoch 42/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2960 - acc: 0.9102 - val_loss: 0.4155 - val_acc: 0.8927\n",
      "Epoch 43/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.3048 - acc: 0.9136 - val_loss: 0.4829 - val_acc: 0.8893\n",
      "Epoch 44/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2890 - acc: 0.9171 - val_loss: 0.4200 - val_acc: 0.8858\n",
      "Epoch 45/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2864 - acc: 0.9240 - val_loss: 0.4052 - val_acc: 0.9031\n",
      "Epoch 46/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2730 - acc: 0.9188 - val_loss: 0.4399 - val_acc: 0.8962\n",
      "Epoch 47/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2601 - acc: 0.9292 - val_loss: 0.4101 - val_acc: 0.8720\n",
      "Epoch 48/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2861 - acc: 0.9257 - val_loss: 0.4456 - val_acc: 0.9031\n",
      "Epoch 49/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2649 - acc: 0.9188 - val_loss: 0.3951 - val_acc: 0.9135\n",
      "Epoch 50/50\n",
      "579/579 [==============================] - 2s 3ms/sample - loss: 0.2674 - acc: 0.9188 - val_loss: 0.4051 - val_acc: 0.8824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa16d7596d8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(\"/home/lauogden/gunshot_cnn_model.h5\")\n",
    "\n",
    "#FIT IT\n",
    "model.fit(train_wav, train_label, \n",
    "          validation_data = [test_wav, test_label],\n",
    "          epochs = 50,\n",
    "          callbacks = model_callbacks\n",
    "          verbose = 1,\n",
    "         batch_size = batch_size,\n",
    "         shuffle = True)\n",
    "\n",
    "model.save(\"/home/lauogden/gunshot_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289,)\n"
     ]
    }
   ],
   "source": [
    "#incorrectly predicted data samples\n",
    "Y_test_pred = model.predict(test_wav)\n",
    "y_predicted_classes_test = y_test_pred.argmax(axis=-1)\n",
    "y_actual_classes_test= test_label.argmax(axis=-1)\n",
    "wrong_examples = np.nonzero(y_predicted_classes_test != y_actual_classes_test)\n",
    "print(wrong_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
